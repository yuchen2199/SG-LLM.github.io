<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SG-LLM</title>
  <link rel="icon" type="image/x-icon" href="static/images/SG-LLM.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SG-LLM: Cooperative Learning from Scene Graphs and Large Language Models for Human-like Driving Scene Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuchen2199.github.io/" target="_blank">Yuchen Zhou</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Xinxin Liu</a>,</span>
                  <span class="author-block">
                  <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/ZHENG-Xinhu/xinhuzheng" target="_blank">Xinhu Zheng</a>,</span>
                  <span class="author-block">
                    <a href="https://chaogou.github.io/" target="_blank">Chao Gou</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Sun Yat-sen University, The Hong Kong University of Science and Technology (Guangzhou)<br>In submission</span>                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3" style="text-align: center;">Video Demonstration</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/EO1EHL5BqG8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <p class="mt-4 is-size-6 has-text-dark">
            Our video demo is also available on 
            <a href="https://www.bilibili.com/video/BV1Xcs1eTE8Q" target="_blank" rel="noopener noreferrer" class="has-text-link">
              bilibili
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Scene understanding is a fundamental component of autonomous driving (AD), yet current data-driven approaches exhibit limitations in addressing real-world cognitive challenges, particularly in terms of interpretability and generalization. In contrast, human drivers efficiently navigate complex environments by leveraging knowledge with minimal training. To bridge this gap, we propose SG-LLM, a novel cooperative learning framework that synergistically integrates high-reliability traffic scene graphs with the extensive knowledge capabilities of large language models (LLMs).
SG-LLM leverages the complementary strengths of scene graphs and LLMs, facilitating both enhanced reliability and improved generalizability.
Furthermore, we introduce a Scene Graph-guided Chain-of-Thought methodology that enables LLMs to perform global-local selective observation and step-by-step reasoning, mimicking human drivers' cognitive processes. 
Extensive experiments on two safety-critical tasks across four diverse datasets demonstrate SG-LLM's effectiveness, even in a 3-shot setting, consistently outperforms previous state-of-the-art methods trained in a fully supervised setting.
SG-LLM establishes a foundational framework for mutual enhancement between scene graph-based and LLM-based knowledge paradigms, advancing AD systems towards human-like scene understanding capabilities.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->











  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
